{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "81ichawkVdnt"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt \n",
        "import gc\n",
        "import sys\n",
        "import math \n",
        "import tensorflow.keras as keras\n",
        "from tensorflow.keras.layers.experimental import preprocessing\n",
        "%matplotlib inline\n",
        "import os\n",
        "import pathlib\n",
        "import transformers \n",
        "import datasets \n",
        "import time \n",
        "import tqdm \n",
        "from tqdm import tqdm \n",
        "import random\n",
        "from sklearn.model_selection import KFold \n",
        "from sklearn.model_selection import StratifiedKFold \n",
        "import tensorflow as tf\n",
        "from transformers import TFAutoModelForSequenceClassification\n"
      ],
      "id": "81ichawkVdnt"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k-TIoKfhVJvv"
      },
      "outputs": [],
      "source": [
        "config = {\n",
        "    'nfolds': 10,\n",
        "    'batch_size': 32,\n",
        "    'learning_rate': 1e-4,\n",
        "    'num_epochs': 3,\n",
        "    'batch_size': 8,\n",
        "}\n",
        "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
        "def seed_all(s):\n",
        "    random.seed(s)\n",
        "    np.random.seed(s)\n",
        "    tf.random.set_seed(s)\n",
        "    os.environ['TF_CUDNN_DETERMINISTIC'] = '1'\n",
        "    os.environ['PYTHONHASHSEED'] = str(s) \n",
        "global_seed = 42\n",
        "seed_all(global_seed)\n",
        "df = pd.read_csv('train.csv')\n",
        "df['y'] = (df[['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']].sum(axis=1) > 0 ).astype(int)\n",
        "df = df[['comment_text', 'y']].rename(columns={'comment_text': 'text'})\n",
        "df['y'].value_counts(normalize=True)\n",
        "\n",
        "min_len = (df['y'] == 1).sum()\n",
        "df_y0_undersample = df[df['y'] == 0].sample(n=min_len, random_state=global_seed)\n",
        "train_df = pd.concat([df[df['y'] == 1], df_y0_undersample]).reset_index(drop=True)\n",
        "\n",
        "n_folds = 10\n",
        "skf = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=global_seed)\n",
        "for nfold, (train_index, val_index) in enumerate(skf.split(X=train_df.index,\n",
        "                                                           y=train_df.y)):\n",
        "    train_df.loc[val_index, 'fold'] = nfold\n",
        "\n",
        "p_fold = 0\n",
        "p_train = train_df.query(f'fold != {p_fold}').reset_index(drop=True)\n",
        "p_valid = train_df.query(f'fold == {p_fold}').reset_index(drop=True)\n",
        "\n",
        "checkpoint = \"bert-base-uncased\"\n",
        "tokenizer = transformers.AutoTokenizer.from_pretrained(checkpoint)\n",
        "\n",
        "train_ds = datasets.Dataset.from_pandas(p_train)\n",
        "valid_ds = datasets.Dataset.from_pandas(p_valid)\n",
        "\n",
        "def tokenize_function(example):\n",
        "    return tokenizer(example[\"text\"], truncation=True)\n",
        "\n",
        "tokenized_train_ds = train_ds.map(tokenize_function, batched=True)\n",
        "tokenized_valid_ds = valid_ds.map(tokenize_function, batched=True)\n",
        "\n",
        "data_collator = transformers.DataCollatorWithPadding(tokenizer=tokenizer)\n",
        "\n",
        "tf_train_ds = tokenized_train_ds.to_tf_dataset(\n",
        "    columns=[\"attention_mask\", \"input_ids\", \"token_type_ids\"],\n",
        "    label_cols=[\"y\"],\n",
        "    shuffle=True,\n",
        "    collate_fn=data_collator,\n",
        "    batch_size=config['batch_size'],\n",
        ")\n",
        "\n",
        "tf_valid_ds = tokenized_valid_ds.to_tf_dataset(\n",
        "    columns=[\"attention_mask\", \"input_ids\", \"token_type_ids\"],\n",
        "    label_cols=[\"y\"],\n",
        "    shuffle=False,\n",
        "    collate_fn=data_collator,\n",
        "    batch_size=config['batch_size'],\n",
        ")\n",
        "model = TFAutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)\n",
        "num_epochs = 2\n",
        "num_train_steps = len(tf_train_ds) * num_epochs\n",
        "\n",
        "lr_scheduler = tf.keras.optimizers.schedules.PolynomialDecay(\n",
        "    initial_learning_rate=5e-5, end_learning_rate=0.0, decay_steps=num_train_steps\n",
        ")\n",
        "\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=lr_scheduler),\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "              metrics=['accuracy'])\n",
        "fit_history = model.fit(tf_train_ds,\n",
        "                        epochs=num_epochs,\n",
        "                        validation_data=tf_valid_ds,\n",
        "                        verbose=1)\n",
        "\n",
        "test_df = pd.read_csv(\"../input/jigsaw-toxic-severity-rating/comments_to_score.csv\")\n",
        "test_ds = datasets.Dataset.from_pandas(test_df)\n",
        "tokenized_test_ds = test_ds.map(tokenize_function, batched=True)\n",
        "tf_test_ds = tokenized_test_ds.to_tf_dataset(\n",
        "    columns=[\"attention_mask\", \"input_ids\", \"token_type_ids\"],\n",
        "    shuffle=False,\n",
        "    collate_fn=data_collator,\n",
        "    batch_size=config['batch_size'],\n",
        ")"
      ],
      "id": "k-TIoKfhVJvv"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install detoxify\n",
        "from detoxify import Detoxify\n",
        "model = Detoxify('original')\n",
        "model.predict(['example text','you are a fucker'])\n",
        "model.predict('you are a fucker')"
      ],
      "metadata": {
        "id": "nXk0o0z4uJba"
      },
      "id": "nXk0o0z4uJba",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "31-W90QHu2lo"
      },
      "id": "31-W90QHu2lo",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "papermill": {
      "default_parameters": {},
      "duration": 2583.209741,
      "end_time": "2021-12-03T06:24:42.759624",
      "environment_variables": {},
      "exception": null,
      "input_path": "__notebook__.ipynb",
      "output_path": "__notebook__.ipynb",
      "parameters": {},
      "start_time": "2021-12-03T05:41:39.549883",
      "version": "2.3.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}