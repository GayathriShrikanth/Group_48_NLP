{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dschCt4G8bfB"
      },
      "source": [
        "# Install libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nq9Eehn8mZ01",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6b533370-7534-4ee1-de61-6ed58c75441f"
      },
      "source": [
        "!pip install tensorflow==1.13.1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tensorflow==1.13.1 in /usr/local/lib/python3.7/dist-packages (1.13.1)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.1) (1.50.0)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.1) (0.8.1)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.1) (3.19.6)\n",
            "Requirement already satisfied: tensorflow-estimator<1.14.0rc0,>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.1) (1.13.0)\n",
            "Requirement already satisfied: tensorboard<1.14.0,>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.1) (1.13.1)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.1) (0.4.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.1) (1.1.2)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.1) (1.21.6)\n",
            "Requirement already satisfied: absl-py>=0.1.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.1) (1.3.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.1) (2.1.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.1) (1.0.8)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.1) (0.38.4)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.1) (1.15.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras-applications>=1.0.6->tensorflow==1.13.1) (3.1.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.1) (3.4.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.1) (1.0.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.1) (4.13.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.1) (3.10.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.1) (4.1.1)\n",
            "Requirement already satisfied: mock>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-estimator<1.14.0rc0,>=1.13.0->tensorflow==1.13.1) (4.0.3)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py->keras-applications>=1.0.6->tensorflow==1.13.1) (1.5.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "swOtuHtAmYkE",
        "outputId": "b33c7b65-20cb-44b8-a08e-0b86cc0141fa"
      },
      "source": [
        "!pip install keras==2.2.4"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: keras==2.2.4 in /usr/local/lib/python3.7/dist-packages (2.2.4)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from keras==2.2.4) (6.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.7/dist-packages (from keras==2.2.4) (1.1.2)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from keras==2.2.4) (1.15.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras==2.2.4) (3.1.0)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.7/dist-packages (from keras==2.2.4) (1.7.3)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.7/dist-packages (from keras==2.2.4) (1.21.6)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.7/dist-packages (from keras==2.2.4) (1.0.8)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py->keras==2.2.4) (1.5.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OfQpKTQNnMJt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "02679205-0614-42e1-da8e-533f26bc1deb"
      },
      "source": [
        "!pip install git+https://www.github.com/keras-team/keras-contrib.git"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting git+https://www.github.com/keras-team/keras-contrib.git\n",
            "  Cloning https://www.github.com/keras-team/keras-contrib.git to /tmp/pip-req-build-m166899c\n",
            "  Running command git clone -q https://www.github.com/keras-team/keras-contrib.git /tmp/pip-req-build-m166899c\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.7/dist-packages (from keras-contrib==2.0.8) (2.2.4)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras->keras-contrib==2.0.8) (3.1.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from keras->keras-contrib==2.0.8) (6.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.7/dist-packages (from keras->keras-contrib==2.0.8) (1.0.8)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from keras->keras-contrib==2.0.8) (1.15.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.7/dist-packages (from keras->keras-contrib==2.0.8) (1.1.2)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.7/dist-packages (from keras->keras-contrib==2.0.8) (1.7.3)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.7/dist-packages (from keras->keras-contrib==2.0.8) (1.21.6)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py->keras->keras-contrib==2.0.8) (1.5.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sHylt6TXGeVN",
        "outputId": "4ba187c3-38d2-467e-9afb-22aa8ce21839"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XsEbmTJsGBca"
      },
      "source": [
        "# Data loader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3sRXbs_PWDAx"
      },
      "source": [
        "# Maximum length of comment\n",
        "max_len = 256\n",
        "# Dimension of embedding vector\n",
        "embedding_dim = 100\n",
        "# Max feature\n",
        "max_feature = 10000"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "data = pd.read_csv(\"/content/drive/MyDrive/NLP File/tsd_train.csv\")"
      ],
      "metadata": {
        "id": "wZEdk-Qw0xT3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Lcz8BrSRC0L"
      },
      "source": [
        "from ast import literal_eval\n",
        "\n",
        "text_data = data['text'].values\n",
        "spans = data['spans'].apply(literal_eval)\n",
        "lbl = [1 if len(s) > 0 else 0 for s in spans]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8O6SVdILRyAh"
      },
      "source": [
        "# Token level \n",
        "\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "import numpy as np\n",
        "\n",
        "tknzr2 = TweetTokenizer()\n",
        "\n",
        "def custom_tokenizer(text_data):\n",
        "    return tknzr2.tokenize(text_data)\n",
        "\n",
        "def retrieve_word_from_span(lst_span, text):\n",
        "    i = 0\n",
        "    token = []\n",
        "    a = 0\n",
        "\n",
        "    word = []\n",
        "\n",
        "    while (i < (len(lst_span) - 1)):\n",
        "        if (lst_span[i] != (lst_span[i+1]-1)):\n",
        "            token.append(lst_span[a:(i+1)])\n",
        "            a = i + 1\n",
        "        elif i == (len(lst_span) - 2):\n",
        "            token.append(lst_span[a:i+2])\n",
        "\n",
        "        i = i + 1\n",
        "\n",
        "    for t in token:\n",
        "        word.append(text[t[0]:(t[len(t)-1])+1])\n",
        "\n",
        "    return word\n",
        "\n",
        "def span_retrived(text_data, spans):\n",
        "    token_labels = []\n",
        "\n",
        "    for i in range(0, len(text_data)):\n",
        "        token_labels.append(retrieve_word_from_span(spans[i], text_data[i]))\n",
        "    \n",
        "    return token_labels\n",
        "\n",
        "def span_convert(text_data, spans):\n",
        "    MAX_LEN = 0\n",
        "    token_labels = []\n",
        "\n",
        "    for i in range(0, len(text_data)):\n",
        "        token_labels.append(retrieve_word_from_span(spans[i], text_data[i]))\n",
        "\n",
        "    lst_seq = []\n",
        "    for i in range(0, len(text_data)):\n",
        "        # token = tknzr.tokenize(text_data[i])\n",
        "        token = custom_tokenizer(text_data[i])\n",
        "        if len(token) > MAX_LEN:\n",
        "            MAX_LEN = len(token)\n",
        "            \n",
        "        seq = np.zeros(len(token), dtype=int)\n",
        "        for j in range(0, len(token)):\n",
        "            for t in token_labels[i]:\n",
        "                # if token[j] in tknzr.tokenize(t):\n",
        "                if token[j] in custom_tokenizer(t):\n",
        "                    seq[j] = 1\n",
        "        lst_seq.append(seq)     \n",
        "\n",
        "    return (token_labels, lst_seq)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jI5OfwZYPaMO"
      },
      "source": [
        "from copy import deepcopy\n",
        "\n",
        "# convert data\n",
        "data['token'], data['seq'] = span_convert(text_data, spans)\n",
        "\n",
        "train = deepcopy(data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dq3YWgcPFkB1"
      },
      "source": [
        "# Word embedding"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "all_words = []\n",
        "embeddings_dictionary = dict()\n",
        "\n",
        "glove_file = open(r\"/content/drive/MyDrive/NLP File/glove.6B.100d.txt\", encoding=\"utf8\")\n",
        "\n",
        "for line in glove_file:\n",
        "    \n",
        "    records = line.split() \n",
        "    word = records[0]\n",
        "    all_words.append(word)\n",
        "    vector_dimensions = np.asarray(records[1:], dtype='float32')\n",
        "    embeddings_dictionary[word] = vector_dimensions\n",
        "    \n",
        "glove_file.close()"
      ],
      "metadata": {
        "id": "epB5qFcE1bfM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "words = all_words\n",
        "num_words = len(words)\n",
        "\n",
        "# Dictionary word:index pair\n",
        "# word is key and its value is corresponding index\n",
        "word_to_index = {w : i + 2 for i, w in enumerate(words)}\n",
        "word_to_index[\"UNK\"] = 1\n",
        "word_to_index[\"PAD\"] = 0\n",
        "\n",
        "# Dictionary lable:index pair\n",
        "idx2word = {i: w for w, i in word_to_index.items()}"
      ],
      "metadata": {
        "id": "E4b2EoXA2bhq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import json\n",
        "# with open('/content/drive/MyDrive/Colab Notebooks/word2index.json', 'w') as fp:\n",
        "#     json.dump(word_to_index, fp)\n",
        "\n",
        "# with open('/content/drive/MyDrive/Colab Notebooks/idx2word.json', 'w') as fp:\n",
        "#     json.dump(idx2word, fp)"
      ],
      "metadata": {
        "id": "FOVssnkiE2L7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_matrix = np.zeros((num_words, embedding_dim))\n",
        "\n",
        "for word, index in word_to_index.items():\n",
        "  if index > max_feature:\n",
        "    continue\n",
        "    \n",
        "  embedding_vector = embeddings_dictionary.get(word)\n",
        "  \n",
        "  if embedding_vector is not None:\n",
        "      embedding_matrix[index] = embedding_vector\n",
        "  \n",
        "  else:\n",
        "      embedding_matrix[index] = np.random.randn(embedding_dim)"
      ],
      "metadata": {
        "id": "XCscZ7Tr2bfq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y = train['seq']\n",
        "x = train['text']"
      ],
      "metadata": {
        "id": "ik2Hfxhc2bda"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train test\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "x_train, x_dev, y_train, y_dev = train_test_split(x, y, test_size = 0.1)"
      ],
      "metadata": {
        "id": "ZgM46HwB2bbB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# from nltk.tokenize import word_tokenize\n",
        "\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.utils import to_categorical\n",
        "from keras.initializers import Constant\n",
        "# from nltk.corpus import stopwords\n",
        "# import re"
      ],
      "metadata": {
        "id": "8ysQhxmf2bYv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "183d74cb-2a45-4ee5-c6d5-3f4b6ebc28a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "Using TensorFlow backend.\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tknzr2 = TweetTokenizer()\n",
        "def custom_tokenizer(text_data):\n",
        "    text_data = text_data.lower()\n",
        "    return tknzr2.tokenize(text_data)"
      ],
      "metadata": {
        "id": "gnlJFqHu5t0A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def encoding(X, y, isTest = True):\n",
        "    sentences = []\n",
        "    \n",
        "    for t in X:\n",
        "        sentences.append(custom_tokenizer(t))\n",
        "\n",
        "    X = []\n",
        "    for s in sentences:\n",
        "        sent = []\n",
        "        for w in s:\n",
        "            try:\n",
        "                w = w.lower()\n",
        "                sent.append(word_to_index[w])\n",
        "            except:\n",
        "                sent.append(word_to_index[\"UNK\"])\n",
        "        X.append(sent)\n",
        "           \n",
        "    X = pad_sequences(maxlen = max_len, sequences = X, padding = \"post\", value = word_to_index[\"PAD\"])\n",
        "\n",
        "    if isTest:\n",
        "        y = pad_sequences(maxlen=max_len, sequences=y, padding=\"post\", value=word_to_index[\"PAD\"])\n",
        "        y = to_categorical(y, num_classes=2)\n",
        "    else:\n",
        "        y = None\n",
        "\n",
        "    return (X,y)"
      ],
      "metadata": {
        "id": "nehSylIh5tx6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x1, y1 = encoding(x_train, y_train)"
      ],
      "metadata": {
        "id": "6_qeY8cA5trs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x2, y2 = encoding(x_dev, y_dev)"
      ],
      "metadata": {
        "id": "zZnw3XCB8buP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(custom_tokenizer(x_train[6194]))\n",
        "print(x_train[6194])\n",
        "print(x1[6194])\n",
        "print(y[6194])"
      ],
      "metadata": {
        "id": "c2-e-crt5tmZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9d40fd7b-497d-4654-ae1e-e6c843a5bd99"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['you', 'must', 'be', 'so', 'dumb', 'that', 'it', 'bypasses', 'you', 'what', 'these', 'immigrants', 'want', '.', 'yes', ',', 'land', 'of', 'the', '\"', 'free', '\"', '.', 'all', 'the', 'freebies', 'from', 'the', 'government', 'they', 'can', 'get', ',', 'because', 'they', \"don't\", 'know', 'english', 'well', '&', 'need', 'an', 'interpreter', '&', 'still', 'pretend', 'not', 'to', 'understand', ',', 'they', 'get', 'benefits', 'that', 'you', '&', 'i', \"aren't\", 'entitled', 'to', 'unless', 'we', 'work', '.', 'go', 'through', 'tsa', 'at', 'the', 'airport', ',', 'they', 'have', 'the', 'average', 'guy', 'almost', 'strip', '&', 'take', 'their', 'belts', 'off', ',', 'almost', 'having', 'to', 'pull', 'down', 'their', 'pants', '.', 'people', 'from', 'other', 'countries', 'wearing', 'hijabs', ',', 'chdors', ',', 'burkas', ',', 'niqabs', ',', 'and', 'other', 'head', 'wraps', ',', 'never', 'have', 'to', 'take', 'them', 'off', '.']\n",
            "You must be so dumb that it bypasses you what these immigrants want.  YES, land of the\" FREE\".  All the freebies from the government they can get, because they don't know English well & need an interpreter & still pretend not to understand, they get benefits that you & I aren't entitled to unless we work.  Go through TSA at the airport, they have the average guy almost strip & take their belts off, almost having to pull down their pants.  People from other countries wearing hijabs, chdors, burkas, niqabs, and other head wraps, never have to take them off.\n",
            "[  132 21624   793     8   160  1011  2200    14    39    16   104  3373\n",
            "    51    50    64     4     7    41  2488    62    15     2   218  3023\n",
            "  7476     4    48     8   211  8310  2201     3    10  4176   852    10\n",
            " 12765  3023     4     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0]\n",
            "[0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.layers import Layer\n",
        "import keras.backend as K\n",
        "# Add attention layer to the deep learning network\n",
        "class attention(Layer):\n",
        "    def __init__(self,**kwargs):\n",
        "        super(attention,self).__init__(**kwargs)\n",
        "\n",
        "    def build(self,input_shape):\n",
        "        self.W=self.add_weight(name='attention_weight', shape=(input_shape[-1],1), \n",
        "                               initializer='random_normal', trainable=True)\n",
        "        self.b=self.add_weight(name='attention_bias', shape=(input_shape[1],1), \n",
        "                               initializer='zeros', trainable=True)        \n",
        "        super(attention, self).build(input_shape)\n",
        "\n",
        "    def call(self,x):\n",
        "        # Alignment scores. Pass them through tanh function\n",
        "        e = K.tanh(K.dot(x,self.W)+self.b)\n",
        "        # Remove dimension of size 1\n",
        "        e = K.squeeze(e, axis=-1)   \n",
        "        # Compute the weights\n",
        "        alpha = K.softmax(e)\n",
        "        # Reshape to tensorFlow format\n",
        "        alpha = K.expand_dims(alpha, axis=-1)\n",
        "        # Compute the context vector\n",
        "        context = x * alpha\n",
        "        context = K.sum(context, axis=1)\n",
        "        return context"
      ],
      "metadata": {
        "id": "nZWsb_KwM4_8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# BiLSTM - CRF \n",
        "from keras.layers import LSTM, Dense, TimeDistributed, Embedding, Bidirectional, Dropout, GlobalMaxPool1D, Input\n",
        "from keras.models import Model\n",
        "from keras_contrib.layers import CRF\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "inp = Input(shape = (max_len,))\n",
        "\n",
        "model = Embedding(input_dim=num_words+2,\n",
        "                    output_dim=embedding_dim,\n",
        "                    embeddings_initializer=Constant(embedding_matrix),\n",
        "                    input_length=max_len,\n",
        "                    trainable=True)(inp)\n",
        "\n",
        "model = Bidirectional(LSTM(units = max_len, return_sequences=True, recurrent_dropout=0.1))(model)\n",
        "\n",
        "# attention_layer = attention()(model)\n",
        "\n",
        "# x = GlobalMaxPool1D()(attention_layer) # reduce dimensionality\n",
        "\n",
        "model = TimeDistributed(Dense(max_len, activation=\"relu\"))(model)\n",
        "\n",
        "# x = Dropout(0.1)(attention_layer)\n",
        "\n",
        "# x = Dense(200, activation = \"sigmoid\")(x)\n",
        "\n",
        "crf = CRF(2)  \n",
        "out = crf(model)\n",
        "\n",
        "model = Model(inp, out)\n",
        "model.compile(optimizer=\"adam\", loss=crf.loss_function, metrics=['accuracy'])\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "Njdi-9u_5tj4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c259fcd6-b9f9-478f-e116-971a32388d34"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "embedding_1 (Embedding)      (None, 256, 100)          40000200  \n",
            "_________________________________________________________________\n",
            "bidirectional_1 (Bidirection (None, 256, 512)          731136    \n",
            "_________________________________________________________________\n",
            "time_distributed_1 (TimeDist (None, 256, 256)          131328    \n",
            "_________________________________________________________________\n",
            "crf_1 (CRF)                  (None, 256, 2)            522       \n",
            "=================================================================\n",
            "Total params: 40,863,186\n",
            "Trainable params: 40,863,186\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prediction"
      ],
      "metadata": {
        "id": "6COvPDNgEb81"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://www.github.com/keras-team/keras-contrib.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gxN7Qx3658N7",
        "outputId": "5f7d63d6-2c18-45e0-8048-b23dc4090ab9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting git+https://www.github.com/keras-team/keras-contrib.git\n",
            "  Cloning https://www.github.com/keras-team/keras-contrib.git to /tmp/pip-req-build-er7g9oln\n",
            "  Running command git clone -q https://www.github.com/keras-team/keras-contrib.git /tmp/pip-req-build-er7g9oln\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.7/dist-packages (from keras-contrib==2.0.8) (2.2.4)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.7/dist-packages (from keras->keras-contrib==2.0.8) (1.0.8)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from keras->keras-contrib==2.0.8) (1.15.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras->keras-contrib==2.0.8) (3.1.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from keras->keras-contrib==2.0.8) (6.0)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.7/dist-packages (from keras->keras-contrib==2.0.8) (1.7.3)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.7/dist-packages (from keras->keras-contrib==2.0.8) (1.1.2)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.7/dist-packages (from keras->keras-contrib==2.0.8) (1.21.6)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py->keras->keras-contrib==2.0.8) (1.5.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def test_prediction(sentence_input):\n",
        "  \n",
        "  sentence_input = custom_tokenizer(sentence_input)\n",
        "  sent = []\n",
        "  for w in sentence_input:\n",
        "      try:\n",
        "          w = w.lower()\n",
        "          sent.append(word_to_index[w])\n",
        "      except:\n",
        "          sent.append(word_to_index[\"UNK\"])\n",
        "  \n",
        "  remaining_pads = max_len - len(sent)\n",
        "  result = sent + [0]*remaining_pads\n",
        "  \n",
        "  test_vector = np.array(result).reshape(1,-1)\n",
        "  y_pred = model.predict(test_vector)\n",
        "  y_pred = np.argmax(y_pred, axis=-1)\n",
        "  \n",
        "  original_test = [idx2word[i] for i in test_vector[0]]\n",
        "  \n",
        "  yy_pred = []\n",
        "  for i in range(0, len(original_test)):\n",
        "      if y_pred[0][i] == 1:\n",
        "        yy_pred.append(original_test[i])\n",
        "  \n",
        "  return yy_pred"
      ],
      "metadata": {
        "id": "YM_KbTpnLHHJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import load_model\n",
        "from keras_contrib.layers import CRF\n",
        "from keras_contrib.losses import crf_loss\n",
        "from keras_contrib.metrics import crf_accuracy\n",
        "\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "import json\n",
        "import ast\n",
        "\n",
        "with open(\"/content/drive/MyDrive/Colab Notebooks/word2index.json\", \"r\") as data:\n",
        "    word_to_index = ast.literal_eval(data.read())\n",
        "\n",
        "with open(\"/content/drive/MyDrive/Colab Notebooks/idx2word.json\", \"r\") as data:\n",
        "    idx2word = ast.literal_eval(data.read())\n",
        "  \n",
        "def custom_tokenizer(text_data):\n",
        "    text_data = text_data.lower()\n",
        "    return tknzr2.tokenize(text_data)\n",
        "\n",
        "max_len = 256\n",
        "\n",
        "model = load_model('/content/drive/MyDrive/Colab Notebooks/new-weights', custom_objects={'CRF':CRF,'crf_loss':crf_loss,'crf_accuracy':crf_accuracy})\n",
        "\n",
        "def test_prediction(sentence_input):\n",
        "  \n",
        "  sentence_input = custom_tokenizer(sentence_input)\n",
        "  sent = []\n",
        "  for w in sentence_input:\n",
        "      try:\n",
        "          w = w.lower()\n",
        "          sent.append(word_to_index[w])\n",
        "      except:\n",
        "          sent.append(word_to_index[\"UNK\"])\n",
        "  \n",
        "  remaining_pads = max_len - len(sent)\n",
        "  result = sent + [0]*remaining_pads\n",
        "  \n",
        "  test_vector = np.array(result).reshape(1,-1)\n",
        "  y_pred = model.predict(test_vector)\n",
        "  y_pred = np.argmax(y_pred, axis=-1)\n",
        "  \n",
        "  original_test = [idx2word[i] for i in test_vector[0]]\n",
        "  \n",
        "  yy_pred = []\n",
        "  for i in range(0, len(original_test)):\n",
        "      if y_pred[0][i] == 1:\n",
        "        yy_pred.append(original_test[i])\n",
        "  \n",
        "  return yy_pred"
      ],
      "metadata": {
        "id": "vOSL73xY2OBP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_prediction(\"You are so disgusting and stupid\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cjuOwtf02DQ6",
        "outputId": "55229b90-64c6-4797-a7d4-c681e863634d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_prediction(\"Fucking asshole, please die\")"
      ],
      "metadata": {
        "id": "MJYKDwCO2y9Y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "261797d3-ed96-4443-aac3-18fc0017716a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_prediction(\"What the fuck are you doing you morons!\")"
      ],
      "metadata": {
        "id": "IUugfCVf3DFm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ca996d1f-b09a-48bd-eadc-bbe56da44c93"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_prediction(\"You stupid fuck\")"
      ],
      "metadata": {
        "id": "vbuYN8tu3Lss",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e1660755-715a-47a9-d74d-3175377cc059"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_prediction(\"What the fuck is going on here\")"
      ],
      "metadata": {
        "id": "nBDjU0v63ZEc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "026e7b44-229f-4e57-805e-1250b72c7b06"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_prediction(\"You black men are slaves to superior whites!!\")"
      ],
      "metadata": {
        "id": "hvNpVCU33mMq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b9986c59-4d7f-44f1-d922-35478a7704c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_prediction(\"You fucking moron\")"
      ],
      "metadata": {
        "id": "GX7UckkK3zEC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5526e3da-154f-47aa-ccd7-a3b4501965c3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_prediction(\"What the fuck are you doing you moron\")"
      ],
      "metadata": {
        "id": "4PrHtTtT4Rh6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pipeline"
      ],
      "metadata": {
        "id": "ou6Bb93BbUuq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "!pip install stanza\n",
        "import stanza\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import re\n",
        "!pip install bs4\n",
        "from bs4 import BeautifulSoup\n",
        "from scipy.sparse import csr_matrix\n",
        "!pip install sparse_dot_topn \n",
        "import sparse_dot_topn.sparse_dot_topn as ct\n",
        "!pip install contractions\n",
        "import contractions\n",
        "#!pip uninstall transformersy\n",
        "!pip install --no-cache-dir transformers sentencepiece\n",
        "from transformers import pipeline\n",
        "import re\n",
        "!pip install -U sentence-transformers\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import torch\n",
        "import itertools\n",
        "import copy\n",
        "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
        "!pip install -U sentence-transformers\n",
        "!pip install detoxify\n",
        "!pip install --upgrade git+https://github.com/flairNLP/flair.git\n",
        "import pandas as pd\n",
        "from detoxify import Detoxify\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from flair.embeddings import FlairEmbeddings\n"
      ],
      "metadata": {
        "id": "xsR2i3bKLQmc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('bad_word.txt') as f:\n",
        "    bad_words =[line.rstrip(\"\\n\") for line in f.readlines()]"
      ],
      "metadata": {
        "id": "CQBLBAUTLQrL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('labeled_comments.csv',  error_bad_lines=False)\n",
        "df.head()"
      ],
      "metadata": {
        "id": "j55YQZfJLQvG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "label =[]\n",
        "for i in range(len(df)):\n",
        "  if df.toxicity_score[i]>0.5:\n",
        "    label.append(1)\n",
        "  else:\n",
        "    label.append(0)\n",
        "df['label'] = label\n",
        "df.head()"
      ],
      "metadata": {
        "id": "714ysQcgLggu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# get only sentence 1 for good sentences\n",
        "for i in range(len(df)):\n",
        "  if df.label[i]==0:\n",
        "    df['comments'][i] = df['comments'][i].split(\".\")[0]"
      ],
      "metadata": {
        "id": "IQ8IZCWtLgj9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def text_prepare(text):\n",
        "    #removing urls\n",
        "    text = re.sub(r'http\\S+', '', text)\n",
        "    text = BeautifulSoup(text).get_text()\n",
        "    #removing emojis\n",
        "    emoji_pattern = re.compile(\"[\"\n",
        "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "                           \"]+\", flags=re.UNICODE)\n",
        "    text = emoji_pattern.sub(r'', text)\n",
        "    text = contractions.fix(text)\n",
        "    #removing email\n",
        "    text =re.sub(r'[A-Za-z0-9]@[A-Za-z]\\.?[A-Za-z0-9]*', \"\", text)\n",
        "    #Keeping only alphabets\n",
        "    regex = re.compile('[^a-zA-Z ]')\n",
        "    text = regex.sub(' ',str(text))\n",
        "    text = re.sub(' +', ' ', text)\n",
        "    text = text.lower()\n",
        "    # delete stopwords from text\n",
        "    # text = ' '.join([word for word in text.split()]) \n",
        "    # text = text.strip()\n",
        "    return text"
      ],
      "metadata": {
        "id": "2gVu9csXLgm9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Data cleaning\n",
        "df['comments']= df['comments'].apply(lambda x:text_prepare(x))"
      ],
      "metadata": {
        "id": "1942PaDnLgpx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# getting good sentences\n",
        "good_sentences = df[df.label==0][:2500].reset_index().drop(['index', 'toxicity_classification', 'toxicity_score', 'label'], axis=1)"
      ],
      "metadata": {
        "id": "99aK3Mn8Lpyv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = stanza.Pipeline(lang='en', processors='tokenize,mwt,pos')"
      ],
      "metadata": {
        "id": "VasZ75_OLqZh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "app = MUDESApp(\"en-large\", use_cuda=False)\n",
        "def replace_BW(sent):\n",
        "  spans = app.predict_toxic_spans(sent, spans=True)\n",
        "  for i in spans[::-1]:\n",
        "    sent = sent[:i[0]]+ 'BW' +sent[i[1]+1:]\n",
        "  return sent"
      ],
      "metadata": {
        "id": "T_Iua84SLqcq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def retrive_pos_tagging_good(balanced_dataset):\n",
        "  pos = []\n",
        "  pos_dict = []\n",
        "  for i in range(len(balanced_dataset)):\n",
        "    doc = nlp(balanced_dataset.comments[i])\n",
        "    temp = []\n",
        "    temp_dict = {}\n",
        "    for sent in doc.sentences:\n",
        "      for word in sent.words:\n",
        "        temp.append(word.xpos)\n",
        "        if word.xpos in temp_dict:\n",
        "          temp_dict[word.xpos].append(word.text)\n",
        "        else:\n",
        "          temp_dict[word.xpos] = [word.text]\n",
        "    pos.append(temp)\n",
        "    pos_dict.append(temp_dict)\n",
        "  return pos, pos_dict"
      ],
      "metadata": {
        "id": "CWhHpw6wLqf1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def retrive_pos_tagging_bad(balanced_dataset):\n",
        "  temp_comments = []\n",
        "  for i in range(len(balanced_dataset)):\n",
        "    sent = balanced_dataset.comments[i]\n",
        "    sent = replace_BW(sent)\n",
        "    temp_comments.append(sent)\n",
        "\n",
        "  balanced_dataset['new_comments'] = temp_comments\n",
        "  pos = []\n",
        "  pos_dict = []\n",
        "  for i in range(len(balanced_dataset)):\n",
        "    doc = nlp(balanced_dataset.new_comments[i])\n",
        "    temp = []\n",
        "    temp_dict = {}\n",
        "    for sent in doc.sentences:\n",
        "      for word in sent.words:\n",
        "        if word.text == 'BW':\n",
        "          temp.append('BW')\n",
        "        else:\n",
        "          temp.append(word.xpos)\n",
        "          if word.xpos in temp_dict:\n",
        "            temp_dict[word.xpos].append(word.text)\n",
        "          else:\n",
        "            temp_dict[word.xpos] = [word.text]\n",
        "    pos.append(temp)\n",
        "    pos_dict.append(temp_dict)\n",
        "  return pos, pos_dict"
      ],
      "metadata": {
        "id": "9WLUh4hsLqjw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pos, pos_dict = retrive_pos_tagging_good(good_sentences)\n",
        "good_sentences['pos'] = pos\n",
        "good_sentences['pos'] = good_sentences['pos'].str.join(\" \")\n",
        "good_sentences['pos_dict'] = pos_dict"
      ],
      "metadata": {
        "id": "g9S7umLrLzSq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_tfidf_vectorizer(good_sentences, bad_sentences):\n",
        "  balanced_dataset = pd.concat([good_sentences, bad_sentences], axis=0, ignore_index=True)\n",
        "  tfidf_vectorizer = TfidfVectorizer(ngram_range=(1,3), max_df=0.9, min_df=5, token_pattern='(\\S+)')\n",
        "  tf_idf_matrix = tfidf_vectorizer.fit(balanced_dataset['pos'])\n",
        "  tf_idf_matrix_good = tfidf_vectorizer.transform(good_sentences['pos'])\n",
        "  return tfidf_vectorizer, tf_idf_matrix_good"
      ],
      "metadata": {
        "id": "bpWjiJFeLzVq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def awesome_cossim_top(A, B, ntop, lower_bound=0):\n",
        "    # force A and B as a CSR matrix.\n",
        "    # If they have already been CSR, there is no overhead\n",
        "    A = A.tocsr()\n",
        "    B = B.tocsr()\n",
        "    M, _ = A.shape\n",
        "    _, N = B.shape\n",
        " \n",
        "    idx_dtype = np.int32\n",
        " \n",
        "    nnz_max = M*ntop\n",
        " \n",
        "    indptr = np.zeros(M+1, dtype=idx_dtype)\n",
        "    indices = np.zeros(nnz_max, dtype=idx_dtype)\n",
        "    data = np.zeros(nnz_max, dtype=A.dtype)\n",
        "    ct.sparse_dot_topn(\n",
        "            M, N, np.asarray(A.indptr, dtype=idx_dtype),\n",
        "            np.asarray(A.indices, dtype=idx_dtype),\n",
        "            A.data,\n",
        "            np.asarray(B.indptr, dtype=idx_dtype),\n",
        "            np.asarray(B.indices, dtype=idx_dtype),\n",
        "            B.data,\n",
        "            ntop,\n",
        "            lower_bound,\n",
        "            indptr, indices, data)\n",
        "    matches = csr_matrix((data,indices,indptr),shape=(M,N)).toarray()\n",
        "    ans =[]\n",
        "    for i in range(len(matches)):\n",
        "      a = matches[i]\n",
        "      ans.append(np.argwhere(a>0))\n",
        "    return ans"
      ],
      "metadata": {
        "id": "vAH9lw3gLzZE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def find_similar_good_sentences(bad_sentence, good_sentences, ans):\n",
        "  similar_df= {'bad':[], 'good':[], 'bad_pos':[], 'good_pos':[], 'bad_pos_map':[], 'good_pos_map':[]}  \n",
        "  similar_df['bad'].append(bad_sentence.comments[0])\n",
        "  similar_df['bad_pos'].append(bad_sentence.pos[0])\n",
        "  similar_df['bad_pos_map'].append(bad_sentence.pos_dict[0])\n",
        "  similar_good = []\n",
        "  similar_pos =[]\n",
        "  similar_pos_map = []\n",
        "  for i in ans:\n",
        "    similar_good.append(good_sentences.comments[i[0]])\n",
        "    similar_pos.append(good_sentences.pos.iloc[i[0]])\n",
        "    similar_pos_map.append(good_sentences.pos_dict[i[0]])\n",
        "  similar_df['good'].append(similar_good)\n",
        "  similar_df['good_pos'].append(similar_pos)\n",
        "  similar_df['good_pos_map'].append(similar_pos_map)\n",
        "  return similar_df"
      ],
      "metadata": {
        "id": "PuY-MvQvLzcL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_assignments(list_1,list_2):\n",
        "  unique_combinations = []\n",
        "  # permut = itertools.permutations(list_1, len(list_2))\n",
        "  # print(permut)\n",
        "\n",
        "  if len(list_1)<len(list_2):\n",
        "      permut = itertools.permutations(list_2, len(list_1))\n",
        "      for comb in permut:\n",
        "        zipped = zip(list_1,comb)\n",
        "        unique_combinations.append(list(zipped))\n",
        "\n",
        "  else:\n",
        "    permut = itertools.permutations(list_1, len(list_2))\n",
        "    for comb in permut:\n",
        "        zipped = zip(comb,list_2)\n",
        "        unique_combinations.append(list(zipped))\n",
        "  \n",
        "  return unique_combinations  \n",
        "def generate(xi, pi, pi_dash, bad_words, pi_map, pi_dash_map):\n",
        "  ti = set(pi)\n",
        "  ti_dash = set(pi_dash)\n",
        "  t_shared = ti.intersection(ti_dash)\n",
        "  c0 = [\"<mask>\"]*len(pi_dash)\n",
        "  ci = [c0]\n",
        "  for inx, tk in enumerate(t_shared):\n",
        "    if tk=='BW':\n",
        "      continue\n",
        "    wk = set(pi_map[tk])\n",
        "    sk = [i for i, x in enumerate(pi_dash) if x == tk]\n",
        "    assignments = make_assignments(wk, sk)\n",
        "    if len(assignments)==0:\n",
        "      continue\n",
        "    new_ci = []\n",
        "    for assignment in assignments:\n",
        "      for prev_sent in ci:\n",
        "        for word, indx in assignment:\n",
        "          temp = copy.deepcopy(prev_sent)\n",
        "          temp[indx]= word\n",
        "        new_ci.append(temp)\n",
        "    ci = new_ci\n",
        "  return ci"
      ],
      "metadata": {
        "id": "X392ond5Lzfa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fill_mask_word_roberta(input_string, classifier_roberta, puncs):\n",
        "    if input_string.split(\" \").count('<mask>') ==0:\n",
        "       return input_string.split(\"[SEP]\",1)[1].strip() \n",
        "    result = classifier_roberta(input_string)\n",
        "    ans = input_string\n",
        "    if ans.split(\" \").count('<mask>') ==1:\n",
        "       result = [result]\n",
        "    for each_mask in result:\n",
        "        for i in range(len(each_mask)):\n",
        "            filler_word = each_mask[i]['token_str'].strip()\n",
        "            if filler_word.lower() not in bad_words and filler_word not in puncs:\n",
        "                ans = re.sub('<mask>', filler_word, ans, count=1)\n",
        "                break\n",
        "            if i == len(each_mask) - 1:\n",
        "                ans = re.sub('<mask>', '', ans, count=1)\n",
        "                break \n",
        "    return ans.split(\"[SEP]\",1)[1].strip()\n"
      ],
      "metadata": {
        "id": "0AJstSW7Lzki"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_adjacent(seq): # works on any sequence, not just on numbers\n",
        "  i = 1\n",
        "  n = len(seq)\n",
        "  while i < n: # avoid calling len(seq) each time around\n",
        "    if seq[i] == seq[i-1]:\n",
        "      del seq[i]\n",
        "      # value returned by seq.pop(i) is ignored; slower than del seq[i]\n",
        "      n -= 1\n",
        "    else:\n",
        "      i += 1"
      ],
      "metadata": {
        "id": "W7knSooFLzpB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def embeddings_cosine_toxicity_perplexity(toxic_model, embedding_model, perplex_model, original, created_list):\n",
        "  \n",
        "  # Here we shall first make the embeddings of the sentences\n",
        "  original_sent_embeddings = embedding_model.encode(original)\n",
        "  created_sent_embeddings = embedding_model.encode(created_list)\n",
        "  my_selection = {\"original_sentence\":original,\"generated_sentences\":created_list, \"cosine_score\":[], \"non-toxicity_score\":[], \"perplexity_score\":[]}\n",
        "  cosine_calc = []\n",
        "  toxic_calc = []\n",
        "  perplex_calc = []\n",
        "\n",
        "  # Now we will calclate the cosine with the original sentences\n",
        "  for indx, i in enumerate(created_sent_embeddings):\n",
        "    toxic_calc.append(1-(toxic_model.predict(created_list[indx])['toxicity']))\n",
        "    perplex_calc.append(perplex_model.calculate_perplexity(created_list[indx])*-1)\n",
        "    cosine_calc.append(cosine_similarity(original_sent_embeddings.reshape(1,-1), i.reshape(1,-1)).astype(str)[0][0])\n",
        "  my_selection.update({\"cosine_score\": cosine_calc})\n",
        "  my_selection.update({\"non-toxicity_score\":toxic_calc})\n",
        "  my_selection.update({\"perplexity_score\":perplex_calc})\n",
        "  \n",
        "  return my_selection"
      ],
      "metadata": {
        "id": "kKuTFO8eLzsT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}